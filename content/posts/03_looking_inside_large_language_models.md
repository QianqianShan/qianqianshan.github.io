---
title: Looking Inside Large Language Models
subtitle:
date: 2025-09-04T03:27:17-07:00
slug: looking_inside_large_language_models
draft: true
author:
  name: Qianqian
  link:
  email:
  avatar:
description:
keywords:
license:
comment: false
weight: 0
tags:
  - hands-on large language models
categories:
  - LLM
hiddenFromHomePage: false
hiddenFromSearch: false
hiddenFromRelated: false
hiddenFromFeed: false
summary:
resources:
  - name: featured-image
    src: featured-image.jpg
  - name: featured-image-preview
    src: featured-image-preview.jpg
toc: true
math: false
lightgallery: false
password:
message:
repost:
  enable: true
  url:

---

<!--more-->

# An Overview of Transformer Models
# The Inputs and Outputs of a Trained Transformer LLM
# The Components of the Forward Pass
# Choosing a Single Token from the Probability Distribution (Sampling/Decoding)
# Parallel Token Processing and Context Size
# Speeding Up Generation by Caching Keys and Values
# Inside the Transformer Block
# Recent Improvements to the Transformer Architecture
# More Efficient Attention
# The Transformer Block
# Positional Embeddings (RoPE)
# Other Architectural Experiments and Improvements
# Summary
