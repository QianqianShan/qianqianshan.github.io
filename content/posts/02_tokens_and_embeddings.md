---
title: Tokens and Embeddings
subtitle:
date: 2025-09-04T03:27:17-07:00
slug: tokens_and_embeddings
draft: true
author:
  name: Qianqian
  link:
  email:
  avatar:
description:
keywords:
license:
comment: false
weight: 0
tags:
  - hands-on large language models
categories:
  - LLM
hiddenFromHomePage: false
hiddenFromSearch: false
hiddenFromRelated: false
hiddenFromFeed: false
summary:
resources:
  - name: featured-image
    src: featured-image.jpg
  - name: featured-image-preview
    src: featured-image-preview.jpg
toc: true
math: false
lightgallery: false
password:
message:
repost:
  enable: true
  url:

---

<!--more-->

# LLM Tokenization
# How Tokenizers Prepare the Inputs to the Language Model
# Downloading and Running an LLM
# How Does the Tokenizer Break Down Text?
# Word Versus Subword Versus Character Versus Byte Tokens
# Comparing Trained LLM Tokenizers
# Tokenizer Properties
# Token Embeddings
# A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer
# Creating Contextualized Word Embeddings with Language Models
# Text Embeddings (for Sentences and Whole Documents)
# Word Embeddings Beyond LLMs
# Using pretrained Word Embeddings
# The Word2vec Algorithm and Contrastive Training
# Embeddings for Recommendation Systems
# Recommending Songs by Embeddings
# Training a Song Embedding Model
# Summary
