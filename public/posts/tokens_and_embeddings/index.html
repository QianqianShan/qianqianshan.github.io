<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/WebPage" lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
    <meta name="robots" content="noodp" />
    <title>2 - Tokens and Embeddings - Random Steps</title><meta name="author" content="Qianqian">
<meta name="description" content=""><meta name="keywords" content='LLM'>
  <meta itemprop="name" content="2 - Tokens and Embeddings">
  <meta itemprop="datePublished" content="2025-09-08T03:27:17-07:00">
  <meta itemprop="dateModified" content="2025-09-08T03:27:17-07:00">
  <meta itemprop="wordCount" content="680">
  <meta itemprop="keywords" content="LLM"><meta property="og:url" content="http://localhost:1313/posts/tokens_and_embeddings/">
  <meta property="og:site_name" content="Random Steps">
  <meta property="og:title" content="2 - Tokens and Embeddings">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-09-08T03:27:17-07:00">
    <meta property="article:modified_time" content="2025-09-08T03:27:17-07:00">
    <meta property="article:tag" content="LLM">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="2 - Tokens and Embeddings">
<meta name="application-name" content="FixIt">
<meta name="apple-mobile-web-app-title" content="FixIt"><meta name="theme-color" data-light="#f8f8f8" data-dark="#252627" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" type="text/html" href="http://localhost:1313/posts/tokens_and_embeddings/" title="2 - Tokens and Embeddings - Random Steps" /><link rel="prev" type="text/html" href="http://localhost:1313/posts/an_introduction_to_large_language_models/" title="1 - An Introduction to Large Language Models" /><link rel="next" type="text/html" href="http://localhost:1313/posts/looking_inside_large_language_models/" title="3 - Looking Inside Large Language Models" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.removeAttribute('onload');this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "2 - Tokens and Embeddings",
    "inLanguage": "en-us",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "http:\/\/localhost:1313\/posts\/tokens_and_embeddings\/"
    },"genre": "posts","keywords": "LLM","wordcount":  680 ,
    "url": "http:\/\/localhost:1313\/posts\/tokens_and_embeddings\/","datePublished": "2025-09-08T03:27:17-07:00","dateModified": "2025-09-08T03:27:17-07:00","publisher": {
      "@type": "Organization",
      "name": ""},"author": {
        "@type": "Person",
        "name": "Qianqian"
      },"description": ""
  }
  </script><script src="/js/head/color-scheme.min.js"></script></head>
  <body data-header-desktop="sticky" data-header-mobile="auto"><div class="wrapper" data-page-style="normal"><header class="desktop animate__faster" id="header-desktop">
  <div class="header-wrapper">
    <div class="header-title">
      <a href="/" title="Random Steps"><span class="header-title-text">Random Steps</span></a><span class="header-subtitle"></span></div>
    <nav>
      <ul class="menu"><li class="menu-item">
              <a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> Archives</a></li><li class="menu-item">
              <a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> Categories</a></li><li class="menu-item">
              <a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> Tags</a></li><li class="menu-item delimiter"></li><li class="menu-item theme-switch" title="Switch Theme">
          <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
        </li></ul>
    </nav>
  </div>
</header><header class="mobile animate__faster" id="header-mobile">
  <div class="header-container">
    <div class="header-wrapper">
      <div class="header-title">
        <a href="/" title="Random Steps"><span class="header-title-text">Random Steps</span></a><span class="header-subtitle"></span></div>
      <div class="menu-toggle" id="menu-toggle-mobile">
        <span></span><span></span><span></span>
      </div>
    </div>
    <nav>
      <ul class="menu" id="menu-mobile"><li class="menu-item"><a class="menu-link" href="/archives/"><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden="true"></i> Archives</a></li><li class="menu-item"><a class="menu-link" href="/categories/"><i class="fa-solid fa-folder-tree fa-fw fa-sm" aria-hidden="true"></i> Categories</a></li><li class="menu-item"><a class="menu-link" href="/tags/"><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden="true"></i> Tags</a></li><li class="menu-item menu-system">
          <span class="menu-system-item theme-switch" title="Switch Theme"><i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i></span></li>
      </ul>
    </nav>
  </div>
</header><main class="container"><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label="Collections"></aside>

  <article class="page single">
    <div class="header"><h1 class="single-title animate__animated animate__flipInX"><span title="Repost" class="icon-repost"><i class="fa-solid fa-share fa-fw" aria-hidden="true"></i></span><span>2 - Tokens and Embeddings</span>
      </h1></div><div class="post-meta">
      <div class="post-meta-line"><span class="post-author"><span class="author"><i class="fa-solid fa-user-circle" aria-hidden="true"></i>
      Qianqian</span></span><span class="post-included-in">&nbsp;included in <a href="/categories/hands-on-large-language-models/" class="post-category" title="Category - Hands-on Large Language Models"><i class="fa-regular fa-folder fa-fw" aria-hidden="true"></i> Hands-on Large Language Models</a></span></div><div class="post-meta-line"><span title="published on 2025-09-08 03:27:17"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden="true"></i><time datetime="2025-09-08">2025-09-08</time></span>&nbsp;<span title="680 words"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden="true"></i>About 700 words</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden="true"></i>4 minutes</span>&nbsp;</div>
    </div><div class="details toc" id="toc-static" data-kept="false">
        <div class="details-summary toc-title">
          <span>Contents</span>
          <span><i class="details-icon fa-solid fa-angle-right" aria-hidden="true"></i></span>
        </div>
        <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#llm-tokenization">LLM Tokenization</a>
      <ul>
        <li><a href="#how-tokenizers-prepare-the-inputs-to-the-language-model">How Tokenizers Prepare the Inputs to the Language Model</a></li>
        <li><a href="#how-does-the-tokenizer-break-down-text">How Does the Tokenizer Break Down Text?</a></li>
        <li><a href="#word-versus-subword-versus-character-versus-byte-tokens">Word Versus Subword Versus Character Versus Byte Tokens</a></li>
      </ul>
    </li>
    <li><a href="#token-embeddings">Token Embeddings</a>
      <ul>
        <li><a href="#a-language-model-holds-embeddings-for-the-vocabulary-of-its-tokenizer">A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer</a></li>
        <li><a href="#creating-contextualized-word-embeddings-with-language-models">Creating Contextualized Word Embeddings with Language Models</a></li>
        <li><a href="#text-embeddings-for-sentences-and-whole-documents">Text Embeddings (for Sentences and Whole Documents)</a></li>
        <li><a href="#word-embeddings-beyond-llms">Word Embeddings Beyond LLMs</a></li>
      </ul>
    </li>
    <li><a href="#embeddings-for-recommendation-systems">Embeddings for Recommendation Systems</a></li>
  </ul>
</nav></div>
      </div><div class="content" id="content"><p>Tokens and embeddings are two central concepts of using LLMs.</p>
<h2 class="heading-element" id="llm-tokenization"><span>LLM Tokenization</span>
  <a href="#llm-tokenization" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><h3 class="heading-element" id="how-tokenizers-prepare-the-inputs-to-the-language-model"><span>How Tokenizers Prepare the Inputs to the Language Model</span>
  <a href="#how-tokenizers-prepare-the-inputs-to-the-language-model" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>Highlevel overview:</p>
<p>Input prompt &ndash;&gt; break it into pieces with tokenizer (token IDs) &ndash;&gt; pass IDs/embeddings to LLM</p>
<figure>
  <img src="/images/hands-on-LLM/02-1.PNG" alt="work-to-embeddings" width="50%" />
  <figcaption>Fig.Tokens and embeddings.</figcaption>
</figure>
<h3 class="heading-element" id="how-does-the-tokenizer-break-down-text"><span>How Does the Tokenizer Break Down Text?</span>
  <a href="#how-does-the-tokenizer-break-down-text" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><table>
  <thead>
      <tr>
          <th>No.</th>
          <th>Phase</th>
          <th>Description</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>1</td>
          <td>Model design time</td>
          <td>Choose a tokenization method (e.g., byte pair encoding used by GPT models, WordPiece by BERT)</td>
      </tr>
      <tr>
          <td>2</td>
          <td>Tokenization design</td>
          <td>Choose the vocabulary size, what special tokens to use</td>
      </tr>
      <tr>
          <td>3</td>
          <td>Training time</td>
          <td>Tokenizer needs to be trained on a <em>specific dataset</em> to establish the best vocabulary of subwords or characters that can accurately and efficiently represent any input text, minimizing the number of &ldquo;unknown&rdquo; tokens that appear (<a href="https://huggingface.co/learn/llm-course/chapter2/4"target="_blank" rel="external nofollow noopener noreferrer">extra reference</a>).</td>
      </tr>
  </tbody>
</table>
<p>When is the tokenizer used?</p>
<ul>
<li>process the <em>input</em> text</li>
<li>decode the token IDs from the <em>output</em> of LLM to words/tokens</li>
</ul>
<h3 class="heading-element" id="word-versus-subword-versus-character-versus-byte-tokens"><span>Word Versus Subword Versus Character Versus Byte Tokens</span>
  <a href="#word-versus-subword-versus-character-versus-byte-tokens" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><table>
  <thead>
      <tr>
          <th>Category</th>
          <th>Description</th>
          <th>Pros</th>
          <th>Cons</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>word tokens</td>
          <td>treat each word as a token</td>
          <td>- suitable for recsys use cases</td>
          <td>- unable to deal with new words after the tokenizer is trained (e.g., apology, apologize)</td>
      </tr>
      <tr>
          <td>subword tokens</td>
          <td>tokens can be full or partial words</td>
          <td>- more expressive vocabulary (e.g., split apology to apolog and suffix tokens -y, -ize, -etic etc.)<br>- able to represent new words by breaking down to smaller characters<br>- fit more text within limited context length</td>
          <td></td>
      </tr>
      <tr>
          <td>character tokens</td>
          <td>split the tokens to characters</td>
          <td>- able to represent new words</td>
          <td>- makes modeling difficult<br>- fit less context length</td>
      </tr>
      <tr>
          <td>byte tokens</td>
          <td>breaks down tokens into individual bytes for unicode chars</td>
          <td>- can be competitive in multi-lingual scenarios</td>
          <td></td>
      </tr>
  </tbody>
</table>
<h2 class="heading-element" id="token-embeddings"><span>Token Embeddings</span>
  <a href="#token-embeddings" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><p>The tokenization convert the language to a sequence of tokens, and the next step is to find the best numerical representation for these tokens, so the model can use them to calculate and model the <em>patterns</em> in the text.</p>
<h3 class="heading-element" id="a-language-model-holds-embeddings-for-the-vocabulary-of-its-tokenizer"><span>A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer</span>
  <a href="#a-language-model-holds-embeddings-for-the-vocabulary-of-its-tokenizer" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>A pre-trained language model is linked with its tokenizer and cannot use a different tokenizer without training. When downloading a pretrained language model, a portion of the model is the embeddings matrix holding the embedding vectors for each token in the tokenizer vocabulary.</p>
<h3 class="heading-element" id="creating-contextualized-word-embeddings-with-language-models"><span>Creating Contextualized Word Embeddings with Language Models</span>
  <a href="#creating-contextualized-word-embeddings-with-language-models" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>To create better token embeddings than the above static ones, language models create <em>contextualized</em> word embeddings to represent a word with a <em>different</em> token based on its context.</p>
<figure>
  <img src="/images/hands-on-LLM/02-9.PNG" alt="Contextual embeddings" width="50%" />
  <figcaption>Fig. Static to contextual embeddings.</figcaption>
</figure>
<h3 class="heading-element" id="text-embeddings-for-sentences-and-whole-documents"><span>Text Embeddings (for Sentences and Whole Documents)</span>
  <a href="#text-embeddings-for-sentences-and-whole-documents" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>A SINGLE vector embeding that represents a a longer text piece (e.g., sentence, documents), and the most common way to produce text embedding vector is average the values of all token embeddings.</p>
<h3 class="heading-element" id="word-embeddings-beyond-llms"><span>Word Embeddings Beyond LLMs</span>
  <a href="#word-embeddings-beyond-llms" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h3><p>Embeddings can also be useful in domains other than LLM such as <em>recommender engines</em> and <em>robotics</em>.</p>
<p>Next let&rsquo;s learn about how the embeddings are generated from the <em>Word2Vec</em> algorithm.</p>
<p>How?</p>
<p>Train a neural network to predict if words commonly apear in the same context or not (i.e., a classification task).</p>
<p>Input:</p>
<ul>
<li>The embeddings of the words</li>
</ul>
<p>Output:</p>
<ul>
<li>The updated embeddings of the words, so next time the model is presented with the vectors, they have a better chance of being correct based on the true labels (if they are true neighbors)</li>
</ul>
<p>Training process:</p>
<p>Inputs embeddings &ndash;&gt; prediction on labels/target value &ndash;&gt; update the embeddings</p>
<p>Key considerations:</p>
<ul>
<li><code>skip-gram</code>: select neighboring words (e.g., use a sliding window to generate training examples: a central word as one input, and its two neighbor words as second inputs, the model will be trained to classify the neighbors as 1 if it is indeed neighbor otherwise 0)</li>
<li><code>negative sampling</code>: enrich the training dataset with examples of words that are NOT typically neighbors</li>
<li>Other tokenization: how to deal with capitalization, punctuation, and how many tokens in vocabulary</li>
</ul>
<h2 class="heading-element" id="embeddings-for-recommendation-systems"><span>Embeddings for Recommendation Systems</span>
  <a href="#embeddings-for-recommendation-systems" class="heading-mark">
    <svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg>
  </a>
</h2><p>RecSys is an example domain that the embeddings can be useful. An example process to make it useful here:</p>
<ol>
<li>Data loading: Playlists from radio stations.</li>
<li>Word2Vec training (treat easch playlist as a sentence, each song in the playlist as a workd/token)</li>
<li>Use the trained embeddings to find similar songs</li>
</ol></div><div class="post-footer" id="post-footer">
  <div class="post-info">
    <div class="post-info-line">
      <div class="post-info-mod">
        <span title="Updated on 2025-09-08 03:27:17">Updated on 2025-09-08&nbsp;</span>
      </div></div><div class="post-info-line">
        <div class="post-info-md"><span><a href="vscode://file//Users/qianqianshan/workspace/MindOrchard/content/posts/02_tokens_and_embeddings.md" title="Open in editor"target="_blank" rel="external nofollow noopener noreferrer" class="link-to-vscode">Open in editor</a></span></div>
        <div class="post-info-share">
          <span><a href="javascript:void(0);" title="Share on X" data-sharer="twitter" data-url="http://localhost:1313/posts/tokens_and_embeddings/" data-title="2 - Tokens and Embeddings" data-hashtags="LLM"><i class="fa-brands fa-x-twitter fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="http://localhost:1313/posts/tokens_and_embeddings/" data-hashtag="LLM"><i class="fa-brands fa-facebook-square fa-fw" aria-hidden="true"></i></a>
  <a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="http://localhost:1313/posts/tokens_and_embeddings/" data-title="2 - Tokens and Embeddings"><i class="fa-brands fa-weibo fa-fw" aria-hidden="true"></i></a>
  </span>
        </div>
      </div></div>

  <div class="post-info-more">
    <section class="post-tags"><i class="fa-solid fa-tags fa-fw me-1" aria-hidden="true"></i><a href="/tags/llm/" class="post-tag" title="Tags - LLM">LLM</a></section>
    <section>
      <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
    </section>
  </div><div class="post-nav"><a href="/posts/an_introduction_to_large_language_models/" class="post-nav-item" rel="prev" title="1 - An Introduction to Large Language Models"><i class="fa-solid fa-angle-left fa-fw" aria-hidden="true"></i>1 - An Introduction to Large Language Models</a><a href="/posts/looking_inside_large_language_models/" class="post-nav-item" rel="next" title="3 - Looking Inside Large Language Models">3 - Looking Inside Large Language Models<i class="fa-solid fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article>

  <aside class="toc" id="toc-auto" aria-label="Contents"><h2 class="toc-title">Contents&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden="true"></i></h2>
      <div class="toc-content" id="toc-content-auto"></div></aside></main><footer class="footer">
    <div class="footer-container"><div class="footer-line powered">Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreferrer" title="Hugo 0.148.2"><img class="hugo-icon" src="/images/hugo.min.svg" alt="Hugo logo" /> Hugo</a> | Theme - <a href="https://github.com/hugo-fixit/FixIt" target="_blank" rel="external" title="FixIt v0.4.0-alpha-20250805041424-57ccd470"><img class="fixit-icon" src="/images/fixit.min.svg" alt="FixIt logo" /> FixIt</a>
        </div><div class="footer-line copyright" itemscope itemtype="http://schema.org/CreativeWork"><i class="fa-regular fa-copyright fa-fw" aria-hidden="true"></i>
            <span itemprop="copyrightYear">2025</span><span class="author" itemprop="copyrightHolder">
              <a href="/"></a></span></div></div>
  </footer></div><div class="widgets"><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role="button" aria-label="Back to Top"><i class="fa-solid fa-arrow-up fa-fw" aria-hidden="true"></i><span class="variant-numeric d-none">0%</span>
        </div></div><div id="mask"></div><noscript>
    <div class="noscript-warning">This website works best with JavaScript enabled.</div>
  </noscript>
</div><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script src="/lib/sharer/sharer.min.js" async defer></script><script src="/lib/cookieconsent/cookieconsent.min.js" defer></script><script>window.config={"code":{"copyTitle":"Copy to clipboard","editLockTitle":"Lock editable code block","editUnLockTitle":"Unlock editable code block","editable":true,"maxShownLines":10},"comment":{"enable":false},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"version":"v0.4.0-alpha-20250805041424-57ccd470"};console.log('Page config:', window.config);</script><script src="/js/theme.min.js" defer></script></body>
</html>
