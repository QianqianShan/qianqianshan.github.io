<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MindOrchard</title>
    <link>https://qianqianshan.github.io/</link>
    <description>MindOrchard</description>
    <generator>Hugo 0.148.2 &amp; FixIt v0.4.0-alpha-20250805041424-57ccd470</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 03:27:17 -0700</lastBuildDate>
    <atom:link href="https://qianqianshan.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>2 - Tokens and Embeddings</title>
      <link>https://qianqianshan.github.io/posts/tokens_and_embeddings/</link>
      <pubDate>Thu, 04 Sep 2025 03:27:17 -0700</pubDate>
      <guid>https://qianqianshan.github.io/posts/tokens_and_embeddings/</guid>
      <category domain="https://qianqianshan.github.io/categories/hands-on-large-language-models/">Hands-on Large Language Models</category>
      <description>&lt;p&gt;Tokens and embeddings are two central concepts of using LLMs.&lt;/p&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;llm-tokenization&#34;&gt;&lt;span&gt;LLM Tokenization&lt;/span&gt;&#xA;  &lt;a href=&#34;#llm-tokenization&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;h3 class=&#34;heading-element&#34; id=&#34;how-tokenizers-prepare-the-inputs-to-the-language-model&#34;&gt;&lt;span&gt;How Tokenizers Prepare the Inputs to the Language Model&lt;/span&gt;&#xA;  &lt;a href=&#34;#how-tokenizers-prepare-the-inputs-to-the-language-model&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Highlevel overview:&lt;/p&gt;&#xA;&lt;p&gt;Input prompt &amp;ndash;&amp;gt; break it into pieces with tokenizer (token IDs) &amp;ndash;&amp;gt; pass IDs/embeddings to LLM&lt;/p&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/02-1.PNG&#34; alt=&#34;work-to-embeddings&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig.Tokens and embeddings.&lt;/div&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;how-does-the-tokenizer-break-down-text&#34;&gt;&lt;span&gt;How Does the Tokenizer Break Down Text?&lt;/span&gt;&#xA;  &lt;a href=&#34;#how-does-the-tokenizer-break-down-text&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;No.&lt;/th&gt;&#xA;          &lt;th&gt;Phase&lt;/th&gt;&#xA;          &lt;th&gt;Description&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Model design time&lt;/td&gt;&#xA;          &lt;td&gt;Choose a tokenization method (e.g., byte pair encoding used by GPT models, WordPiece by BERT)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Tokenization design&lt;/td&gt;&#xA;          &lt;td&gt;Choose the vocabulary size, what special tokens to use&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Training time&lt;/td&gt;&#xA;          &lt;td&gt;Tokenizer needs to be trained on a &lt;em&gt;specific dataset&lt;/em&gt; to establish the best vocabulary of subwords or characters that can accurately and efficiently represent any input text, minimizing the number of &amp;ldquo;unknown&amp;rdquo; tokens that appear (&lt;a href=&#34;https://huggingface.co/learn/llm-course/chapter2/4&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;extra reference&lt;/a&gt;).&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;When is the tokenizer used?&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;process the &lt;em&gt;input&lt;/em&gt; text&lt;/li&gt;&#xA;&lt;li&gt;decode the token IDs from the &lt;em&gt;output&lt;/em&gt; of LLM to words/tokens&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;word-versus-subword-versus-character-versus-byte-tokens&#34;&gt;&lt;span&gt;Word Versus Subword Versus Character Versus Byte Tokens&lt;/span&gt;&#xA;  &lt;a href=&#34;#word-versus-subword-versus-character-versus-byte-tokens&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Category&lt;/th&gt;&#xA;          &lt;th&gt;Description&lt;/th&gt;&#xA;          &lt;th&gt;Pros&lt;/th&gt;&#xA;          &lt;th&gt;Cons&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;word tokens&lt;/td&gt;&#xA;          &lt;td&gt;treat each word as a token&lt;/td&gt;&#xA;          &lt;td&gt;- suitable for recsys use cases&lt;/td&gt;&#xA;          &lt;td&gt;- unable to deal with new words after the tokenizer is trained (e.g., apology, apologize)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;subword tokens&lt;/td&gt;&#xA;          &lt;td&gt;tokens can be full or partial words&lt;/td&gt;&#xA;          &lt;td&gt;- more expressive vocabulary (e.g., split apology to apolog and suffix tokens -y, -ize, -etic etc.)&lt;br&gt;- able to represent new words by breaking down to smaller characters&lt;br&gt;- fit more text within limited context length&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;character tokens&lt;/td&gt;&#xA;          &lt;td&gt;split the tokens to characters&lt;/td&gt;&#xA;          &lt;td&gt;- able to represent new words&lt;/td&gt;&#xA;          &lt;td&gt;- makes modeling difficult&lt;br&gt;- fit less context length&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;byte tokens&lt;/td&gt;&#xA;          &lt;td&gt;breaks down tokens into individual bytes for unicode chars&lt;/td&gt;&#xA;          &lt;td&gt;- can be competitive in multi-lingual scenarios&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;token-embeddings&#34;&gt;&lt;span&gt;Token Embeddings&lt;/span&gt;&#xA;  &lt;a href=&#34;#token-embeddings&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;The tokenization convert the language to a sequence of tokens, and the next step is to find the best numerical representation for these tokens, so the model can use them to calculate and model the &lt;em&gt;patterns&lt;/em&gt; in the text.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;a-language-model-holds-embeddings-for-the-vocabulary-of-its-tokenizer&#34;&gt;&lt;span&gt;A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer&lt;/span&gt;&#xA;  &lt;a href=&#34;#a-language-model-holds-embeddings-for-the-vocabulary-of-its-tokenizer&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;A pre-trained language model is linked with its tokenizer and cannot use a different tokenizer without training. When downloading a pretrained language model, a portion of the model is the embeddings matrix holding the embedding vectors for each token in the tokenizer vocabulary.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;creating-contextualized-word-embeddings-with-language-models&#34;&gt;&lt;span&gt;Creating Contextualized Word Embeddings with Language Models&lt;/span&gt;&#xA;  &lt;a href=&#34;#creating-contextualized-word-embeddings-with-language-models&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;To create better token embeddings than the above static ones, language models create &lt;em&gt;contextualized&lt;/em&gt; word embeddings to represent a word with a &lt;em&gt;different&lt;/em&gt; token based on its context.&lt;/p&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/02-9.PNG&#34; alt=&#34;Contextual embeddings&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig. Static to contextual embeddings.&lt;/div&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;text-embeddings-for-sentences-and-whole-documents&#34;&gt;&lt;span&gt;Text Embeddings (for Sentences and Whole Documents)&lt;/span&gt;&#xA;  &lt;a href=&#34;#text-embeddings-for-sentences-and-whole-documents&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;A SINGLE vector embeding that represents a a longer text piece (e.g., sentence, documents), and the most common way to produce text embedding vector is average the values of all token embeddings.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;word-embeddings-beyond-llms&#34;&gt;&lt;span&gt;Word Embeddings Beyond LLMs&lt;/span&gt;&#xA;  &lt;a href=&#34;#word-embeddings-beyond-llms&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Embeddings can also be useful in domains other than LLM such as &lt;em&gt;recommender engines&lt;/em&gt; and &lt;em&gt;robotics&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Next let&amp;rsquo;s learn about how the embeddings are generated from the &lt;em&gt;Word2Vec&lt;/em&gt; algorithm.&lt;/p&gt;&#xA;&lt;p&gt;How?&lt;/p&gt;&#xA;&lt;p&gt;Train a neural network to predict if words commonly apear in the same context or not (i.e., a classification task).&lt;/p&gt;&#xA;&lt;p&gt;Input:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The embeddings of the words&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Output:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The updated embeddings of the words, so next time the model is presented with the vectors, they have a better chance of being correct based on the true labels (if they are true neighbors)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Training process:&lt;/p&gt;&#xA;&lt;p&gt;Inputs embeddings &amp;ndash;&amp;gt; prediction on labels/target value &amp;ndash;&amp;gt; update the embeddings&lt;/p&gt;&#xA;&lt;p&gt;Key considerations:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;skip-gram&lt;/code&gt;: select neighboring words (e.g., use a sliding window to generate training examples: a central word as one input, and its two neighbor words as second inputs, the model will be trained to classify the neighbors as 1 if it is indeed neighbor otherwise 0)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;negative sampling&lt;/code&gt;: enrich the training dataset with examples of words that are NOT typically neighbors&lt;/li&gt;&#xA;&lt;li&gt;Other tokenization: how to deal with capitalization, punctuation, and how many tokens in vocabulary&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;embeddings-for-recommendation-systems&#34;&gt;&lt;span&gt;Embeddings for Recommendation Systems&lt;/span&gt;&#xA;  &lt;a href=&#34;#embeddings-for-recommendation-systems&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;RecSys is an example domain that the embeddings can be useful. An example process to make it useful here:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Data loading: Playlists from radio stations.&lt;/li&gt;&#xA;&lt;li&gt;Word2Vec training (treat easch playlist as a sentence, each song in the playlist as a workd/token)&lt;/li&gt;&#xA;&lt;li&gt;Use the trained embeddings to find similar songs&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>1 - An Introduction to Large Language Models</title>
      <link>https://qianqianshan.github.io/posts/an_introduction_to_large_language_models/</link>
      <pubDate>Wed, 03 Sep 2025 03:27:17 -0700</pubDate>
      <guid>https://qianqianshan.github.io/posts/an_introduction_to_large_language_models/</guid>
      <category domain="https://qianqianshan.github.io/categories/hands-on-large-language-models/">Hands-on Large Language Models</category>
      <description>&lt;h2 class=&#34;heading-element&#34; id=&#34;what-is-language-ai&#34;&gt;&lt;span&gt;What Is Language AI?&lt;/span&gt;&#xA;  &lt;a href=&#34;#what-is-language-ai&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;Language AI is a subfield of AI that focuses  on developing technologies capable of understanding, processing, and generating human language.&lt;/p&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;a-recent-history-of-language-ai&#34;&gt;&lt;span&gt;A Recent History of Language AI&lt;/span&gt;&#xA;  &lt;a href=&#34;#a-recent-history-of-language-ai&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;h3 class=&#34;heading-element&#34; id=&#34;representing-language-as-a-bag-of-words&#34;&gt;&lt;span&gt;Representing Language as a Bag-of-Words&lt;/span&gt;&#xA;  &lt;a href=&#34;#representing-language-as-a-bag-of-words&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Tokenization: split up the sentence into individual words or subwords (tokens)&lt;/li&gt;&#xA;&lt;li&gt;Create a vocabulary: combine all unique words from each sentence to create the vocabulary that can be used to represent the sentences&lt;/li&gt;&#xA;&lt;li&gt;Create a bag-of-words by counting how often a word in each sentence appears&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;That is, a bag-of-words model create representations of text in the form of numbers (or vectors), but it &lt;em&gt;ignores the semantic nature or the meaning of the text by considering language as a bag of words&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/01-5.PNG&#34; alt=&#34;Bag-of-words&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig. Bag-of-words flow.&lt;/div&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;better-representations-with-dense-vector-embeddings&#34;&gt;&lt;span&gt;Better Representations with Dense Vector Embeddings&lt;/span&gt;&#xA;  &lt;a href=&#34;#better-representations-with-dense-vector-embeddings&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;&lt;em&gt;word2vec&lt;/em&gt; (released in 2013) attempts to capture the meaning of text in embeddings.&lt;/p&gt;&#xA;&lt;p&gt;Steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use neural network with interconnected layers of nodes to process info.&lt;/li&gt;&#xA;&lt;li&gt;Assign each word a vector embedding with initialized random values.&lt;/li&gt;&#xA;&lt;li&gt;Take pairs of words from the training data, and train model to predict whether they are likely to be neighbors in a sentence.&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;word2vec&lt;/em&gt; learns the relationship between words and distills the info into the embedding.&lt;/li&gt;&#xA;&lt;li&gt;Now if two words tend to have the same neighbors, their embeddings will be closer.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Embeddings attempt to capture meaning by &lt;em&gt;representing the properties of words&lt;/em&gt;, for example, &amp;ldquo;baby&amp;rdquo; may score high on the properties &amp;ldquo;newborn&amp;rdquo; and &amp;ldquo;human&amp;rdquo;, and score low in properties &amp;ldquo;fruit&amp;rdquo;. In practice, the properties are often obscure and seldom relate to a single humanly identifiable concept, but makes sense to translate human language to compute language.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;types-of-embeddings&#34;&gt;&lt;span&gt;Types of Embeddings&lt;/span&gt;&#xA;  &lt;a href=&#34;#types-of-embeddings&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Different levels of embeddings indicate different levels of abstractions (word vs sentence).&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;word embeddings (e.g., word2vec)&lt;/li&gt;&#xA;&lt;li&gt;sentence embeddings&lt;/li&gt;&#xA;&lt;li&gt;document embeddings (e.g., bag-of-words)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;encoding-and-decoding-context-with-attention&#34;&gt;&lt;span&gt;Encoding and Decoding Context with Attention&lt;/span&gt;&#xA;  &lt;a href=&#34;#encoding-and-decoding-context-with-attention&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;How to switch to the dynamic embeddings so the meaning of words is dependent on its &lt;em&gt;context&lt;/em&gt;? For example, &amp;ldquo;bank&amp;rdquo; can refer a financial bank, or the bank of a river.&lt;/p&gt;&#xA;&lt;p&gt;Method 1: Recurrent neural networks (RNNs) - it models the word sequences as an additional input. It encodes to represent an input sentence, and decodes to generate an output sentence. However, it is difficult to deal with longer sentences with one single embedding to represent the entire input.&lt;/p&gt;&#xA;&lt;p&gt;Method 2: Attention feature - To solve the long sentence embedding issue of RNN, attention is added to the &lt;em&gt;decoder step&lt;/em&gt;. It selectively determines which words are most important in a given sentence, and allows a model to focus on parts of the input sequence that are relevant. That is, the input of the generation/decoding step is now the (1). context embedding generated by encoder (2). the hidden states of all input words as signal for each input word related to the potential output.&lt;/p&gt;&#xA;&lt;p&gt;Drawback: Method 1 + 2 has a sequential nature and prevents parallelization during the training of the model.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;attention-is-all-you-need&#34;&gt;&lt;span&gt;Attention Is All You Need&lt;/span&gt;&#xA;  &lt;a href=&#34;#attention-is-all-you-need&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;A network architecture called &lt;em&gt;Transformer&lt;/em&gt; is&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;solely based on the attention mechanism&lt;/li&gt;&#xA;&lt;li&gt;removed the recurrent network, which makes it ideal for parallel training&lt;/li&gt;&#xA;&lt;li&gt;remains autoregressive - consume each generated word before creating a new word&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Two blocks in the transformer architecture: encoder and decoder&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Common layers: self-attention and feedforward neural network&lt;/li&gt;&#xA;&lt;li&gt;Unique layer of decoder: a layer to pay attention to the output of the encoder&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/01-17.PNG&#34; alt=&#34;Encoder block&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig. Encoder block of transformer.&lt;/div&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/01-19.PNG&#34; alt=&#34;Decoder block&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig. Decoder block of transformer.&lt;/div&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;representation-models-encoder-only-models&#34;&gt;&lt;span&gt;Representation Models: Encoder-Only Models&lt;/span&gt;&#xA;  &lt;a href=&#34;#representation-models-encoder-only-models&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Bidirectional Encoder Representations (BERT) is an encoder-only architecture that focuses on &lt;em&gt;representing language&lt;/em&gt;, and the encoder stacks are training with &lt;em&gt;masked language modeling&lt;/em&gt; - masks part of the input for the model to predict, to allow BERT to create better representations of the input.&lt;/p&gt;&#xA;&lt;p&gt;Use case:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Transfer learning: First pretrain (e.g., on entire wikipedia) and then fine-tune it for a specific task (e.g., classification, named entity recognition, clustering, semantic search, etc.)&lt;/li&gt;&#xA;&lt;li&gt;Feature extraction machine: BERT-like models generate embeddings at almost every step in their architecture, thus making extracting features possible without the fine-tuning for a specific task.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;generative-models-decoder-only-models&#34;&gt;&lt;span&gt;Generative Models: Decoder-Only Models&lt;/span&gt;&#xA;  &lt;a href=&#34;#generative-models-decoder-only-models&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Focus on generating text like GPT.&lt;/p&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;the-training-paradigm-of-large-language-models&#34;&gt;&lt;span&gt;The Training Paradigm of Large Language Models&lt;/span&gt;&#xA;  &lt;a href=&#34;#the-training-paradigm-of-large-language-models&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;For tradition machine learning tasks, it usually involves training a model for a specific task (e.g., classification) with one-step process. In contrast, creating LLMs usually consists of two steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Language modeling (pretraining): Train on a vast corpus of internet text allowing model to learn grammer, context, and language patterns. The output model is a &lt;em&gt;foundational model&lt;/em&gt; or &lt;em&gt;base model&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Fine-tuning (post-training): Further train the previous model on a narrower task.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;large-language-model-applications-what-makes-them-so-useful&#34;&gt;&lt;span&gt;Large Language Model Applications: What Makes Them So Useful?&lt;/span&gt;&#xA;  &lt;a href=&#34;#large-language-model-applications-what-makes-them-so-useful&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;Common tasks and techniques:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Detecting review sentiment (supervised learning)&lt;/li&gt;&#xA;&lt;li&gt;Developing system to find common topics in ticket issues (unsupervised learning)&lt;/li&gt;&#xA;&lt;li&gt;Building a system for retrieval and inspection of relevant documents (e.g., RAG)&lt;/li&gt;&#xA;&lt;li&gt;Constructing chatbot with external resources&lt;/li&gt;&#xA;&lt;li&gt;Constructing LLM of writing recipes based on fridge food picture (multi-modality)&lt;/li&gt;&#xA;&lt;li&gt;Many more&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Hello</title>
      <link>https://qianqianshan.github.io/posts/b12471f/</link>
      <pubDate>Sun, 10 Aug 2025 23:50:24 -0700</pubDate>
      <guid>https://qianqianshan.github.io/posts/b12471f/</guid>
      <category domain="https://qianqianshan.github.io/categories/draft/">Draft</category>
      <description></description>
    </item>
  </channel>
</rss>
