<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Random Steps</title>
    <link>https://qianqianshan.github.io/</link>
    <description>Random Steps</description>
    <generator>Hugo 0.148.2 &amp; FixIt v0.4.0-alpha-20250805041424-57ccd470</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 03:27:17 -0700</lastBuildDate>
    <atom:link href="https://qianqianshan.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>3 - Looking Inside Large Language Models</title>
      <link>https://qianqianshan.github.io/posts/looking_inside_large_language_models/</link>
      <pubDate>Tue, 09 Sep 2025 03:27:17 -0700</pubDate>
      <guid>https://qianqianshan.github.io/posts/looking_inside_large_language_models/</guid>
      <category domain="https://qianqianshan.github.io/categories/hands-on-large-language-models/">Hands-on Large Language Models</category>
      <description>&lt;h2 class=&#34;heading-element&#34; id=&#34;an-overview-of-transformer-models&#34;&gt;&lt;span&gt;An Overview of Transformer Models&lt;/span&gt;&#xA;  &lt;a href=&#34;#an-overview-of-transformer-models&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;h3 class=&#34;heading-element&#34; id=&#34;the-components-of-the-forward-pass&#34;&gt;&lt;span&gt;The Components of the Forward Pass&lt;/span&gt;&#xA;  &lt;a href=&#34;#the-components-of-the-forward-pass&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Tokenizer: breaks down text into a sequence of token IDs that becomes the input to the model&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Transformer blocks: processes the input with the vector representations (embeddings) associated with each of the input token&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Language modeling (LM) head: translate output into probability scores on what the most likely next token is. Done with a simple neural network layer&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/03-5.PNG&#34; alt=&#34;forwardpasscomponent&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig.The tokenizer, transformer, and LM head components of the forward pass.&lt;/div&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;choosing-a-single-token-from-the-probability-distribution-samplingdecoding&#34;&gt;&lt;span&gt;Choosing a Single Token from the Probability Distribution (Sampling/Decoding)&lt;/span&gt;&#xA;  &lt;a href=&#34;#choosing-a-single-token-from-the-probability-distribution-samplingdecoding&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Greedy decoding: always pick the token with highest probability score  - usually tend NOT to lead to the best outputs for most use cases&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Sampling: add some randomness and sample from the probability distribution based on the probability score&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;parallel-token-processing-and-context-size&#34;&gt;&lt;span&gt;Parallel Token Processing and Context Size&lt;/span&gt;&#xA;  &lt;a href=&#34;#parallel-token-processing-and-context-size&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;Parallel token: The tokenizer will break down the text into tokens, and each token flows through its own computation path.&lt;/p&gt;&#xA;&lt;p&gt;Context size: the limit of how many tokens the transformer model can process at once.&lt;/p&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;speeding-up-generation-by-caching-keys-and-values&#34;&gt;&lt;span&gt;Speeding Up Generation by Caching Keys and Values&lt;/span&gt;&#xA;  &lt;a href=&#34;#speeding-up-generation-by-caching-keys-and-values&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;Keys and values cache: Cache the results of the previous calculation (esp. the specific vectors in the attention mechanism), so we don&amp;rsquo;t need to repeat the calculations of the previous streams when generating the next token, and only need to calculate the last stream.&lt;/p&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;inside-the-transformer-block&#34;&gt;&lt;span&gt;Inside the Transformer Block&lt;/span&gt;&#xA;  &lt;a href=&#34;#inside-the-transformer-block&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;  &lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/03-12.PNG&#34; alt=&#34;TransformerBlock&#34; width=&#34;50%&#34; /&gt;&#xA;  &lt;div style=&#34;text-align: left;&#34;&gt;Fig.The Transformer block components.&lt;/div&gt;&#xA;&lt;p&gt;A transformer block has two successive components:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The &lt;em&gt;attention&lt;/em&gt; layer: incorporate relevant info from other input tokens and positions&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Attention is a mechanism that helps the model incorporate &lt;em&gt;context&lt;/em&gt; as it&amp;rsquo;s processing a specific token.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Two steps in the attention mechanism:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;Relevance scoring&lt;/em&gt; of the previous input tokens to the current token&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Information combination&lt;/em&gt; of the various positions into a single output vector&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The above attention mechanism is &lt;em&gt;duplicated and executed&lt;/em&gt; multiple times in parallel in &lt;em&gt;attention head&lt;/em&gt; to increase the model capacity to model complex patterns that require paying attention to different patterns at once.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The training process produces 3 projection matrices that produce the components that interact in this calculation: query, key, and value. (Qianqian: More details about this will be discussed in later blogs on attention).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;  &lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/03-17.PNG&#34; alt=&#34;MultipleTimesOfTheAttentionMechanism&#34; width=&#34;50%&#34; /&gt;&#xA;  &lt;div style=&#34;text-align: left;&#34;&gt;Fig.Doing attention multiple times in parallel with the relevance scoring and info combination steps.&lt;/div&gt;&#xA;&lt;ol start=&#34;2&#34;&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;The &lt;em&gt;feedforward&lt;/em&gt; layer: the majority of the model processing capacity&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Intuition of feedforward NN: if we pass the simple input &amp;ldquo;The Shawshank&amp;rdquo; to the model, it is expected to generate &amp;ldquo;Redemption&amp;rdquo; as the most probable next word.&lt;/li&gt;&#xA;&lt;li&gt;During the training process, it learned and stored the info and behaviors that make it succeed at this task.&lt;/li&gt;&#xA;&lt;li&gt;In can both &lt;strong&gt;memorize&lt;/strong&gt; and &lt;strong&gt;interpolate&lt;/strong&gt; between data points and more complext patterns to be able to generalize on inputs it hadn&amp;rsquo;t seen before.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;recent-improvements-to-the-transformer-architecture&#34;&gt;&lt;span&gt;Recent Improvements to the Transformer Architecture&lt;/span&gt;&#xA;  &lt;a href=&#34;#recent-improvements-to-the-transformer-architecture&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;How to improve and create better models:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Train on larger datasets&lt;/li&gt;&#xA;&lt;li&gt;Optimizations for the training process and learning reates to use&lt;/li&gt;&#xA;&lt;li&gt;The architecture improvements&lt;/li&gt;&#xA;&lt;li&gt;&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;more-efficient-attention&#34;&gt;&lt;span&gt;More Efficient Attention&lt;/span&gt;&#xA;  &lt;a href=&#34;#more-efficient-attention&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;ul&gt;&#xA;&lt;li&gt;Local/sparse attention: limit the contexxt of previous tokens that the model can attend to&lt;/li&gt;&#xA;&lt;li&gt;Multi-query and group-query attention: share the keys and values matrices between all the heads (multi-query) or between the original multi-head and multi-query method to use different groups of keys and values (group-query) to improve inference scalability of larger models by reducing the size of the matrics involved&lt;/li&gt;&#xA;&lt;li&gt;Flash attention: optimize what values are loaded and moved between a GPU shared memory and high bandwidth memory&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;improvement-on-other-parts-of-the-transformer-block&#34;&gt;&lt;span&gt;Improvement on Other Parts of The Transformer Block&lt;/span&gt;&#xA;  &lt;a href=&#34;#improvement-on-other-parts-of-the-transformer-block&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;  &lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/03-29-paper.PNG&#34; alt=&#34;OriginalTransformerPaperArchitecture&#34; width=&#34;50%&#34; /&gt;&#xA;  &lt;div style=&#34;text-align: left;&#34;&gt;Fig.The original Transformer model architecture (reference: https://arxiv.org/pdf/1706.03762).&lt;/div&gt;&#xA;&lt;p&gt;Other than the attention layer and feedforward neural network, the transformer block also has &lt;em&gt;add and normalize&lt;/em&gt; as seen in the transformer block diagram from the original transformer paper. Improvements were made as follows:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Normalization can happen before attention and feedfoward layers to reduce the training time.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Can use RMSNorm, a simpler and more effieicnet way than LayerNorm in the original paper.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Newer activation function variants such as SwiGLU than the ReLU activation function in the original paper.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;positional-embeddings-rope&#34;&gt;&lt;span&gt;Positional Embeddings (RoPE)&lt;/span&gt;&#xA;  &lt;a href=&#34;#positional-embeddings-rope&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Use rotary positional embeddings instead of the absolte ones to encode positional info ina way that captures absolute and relative token position info.&lt;/p&gt;</description>
    </item>
    <item>
      <title>2 - Tokens and Embeddings</title>
      <link>https://qianqianshan.github.io/posts/tokens_and_embeddings/</link>
      <pubDate>Mon, 08 Sep 2025 03:27:17 -0700</pubDate>
      <guid>https://qianqianshan.github.io/posts/tokens_and_embeddings/</guid>
      <category domain="https://qianqianshan.github.io/categories/hands-on-large-language-models/">Hands-on Large Language Models</category>
      <description>&lt;p&gt;Tokens and embeddings are two central concepts of using LLMs.&lt;/p&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;llm-tokenization&#34;&gt;&lt;span&gt;LLM Tokenization&lt;/span&gt;&#xA;  &lt;a href=&#34;#llm-tokenization&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;h3 class=&#34;heading-element&#34; id=&#34;how-tokenizers-prepare-the-inputs-to-the-language-model&#34;&gt;&lt;span&gt;How Tokenizers Prepare the Inputs to the Language Model&lt;/span&gt;&#xA;  &lt;a href=&#34;#how-tokenizers-prepare-the-inputs-to-the-language-model&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Highlevel overview:&lt;/p&gt;&#xA;&lt;p&gt;Input prompt &amp;ndash;&amp;gt; break it into pieces with tokenizer (token IDs) &amp;ndash;&amp;gt; pass IDs/embeddings to LLM&lt;/p&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/02-1.PNG&#34; alt=&#34;work-to-embeddings&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig.Tokens and embeddings.&lt;/div&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;how-does-the-tokenizer-break-down-text&#34;&gt;&lt;span&gt;How Does the Tokenizer Break Down Text?&lt;/span&gt;&#xA;  &lt;a href=&#34;#how-does-the-tokenizer-break-down-text&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;No.&lt;/th&gt;&#xA;          &lt;th&gt;Phase&lt;/th&gt;&#xA;          &lt;th&gt;Description&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Model design time&lt;/td&gt;&#xA;          &lt;td&gt;Choose a tokenization method (e.g., byte pair encoding used by GPT models, WordPiece by BERT)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;2&lt;/td&gt;&#xA;          &lt;td&gt;Tokenization design&lt;/td&gt;&#xA;          &lt;td&gt;Choose the vocabulary size, what special tokens to use&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;3&lt;/td&gt;&#xA;          &lt;td&gt;Training time&lt;/td&gt;&#xA;          &lt;td&gt;Tokenizer needs to be trained on a &lt;em&gt;specific dataset&lt;/em&gt; to establish the best vocabulary of subwords or characters that can accurately and efficiently represent any input text, minimizing the number of &amp;ldquo;unknown&amp;rdquo; tokens that appear (&lt;a href=&#34;https://huggingface.co/learn/llm-course/chapter2/4&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;extra reference&lt;/a&gt;).&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;When is the tokenizer used?&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;process the &lt;em&gt;input&lt;/em&gt; text&lt;/li&gt;&#xA;&lt;li&gt;decode the token IDs from the &lt;em&gt;output&lt;/em&gt; of LLM to words/tokens&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;word-versus-subword-versus-character-versus-byte-tokens&#34;&gt;&lt;span&gt;Word Versus Subword Versus Character Versus Byte Tokens&lt;/span&gt;&#xA;  &lt;a href=&#34;#word-versus-subword-versus-character-versus-byte-tokens&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Category&lt;/th&gt;&#xA;          &lt;th&gt;Description&lt;/th&gt;&#xA;          &lt;th&gt;Pros&lt;/th&gt;&#xA;          &lt;th&gt;Cons&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;word tokens&lt;/td&gt;&#xA;          &lt;td&gt;treat each word as a token&lt;/td&gt;&#xA;          &lt;td&gt;- suitable for recsys use cases&lt;/td&gt;&#xA;          &lt;td&gt;- unable to deal with new words after the tokenizer is trained (e.g., apology, apologize)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;subword tokens&lt;/td&gt;&#xA;          &lt;td&gt;tokens can be full or partial words&lt;/td&gt;&#xA;          &lt;td&gt;- more expressive vocabulary (e.g., split apology to apolog and suffix tokens -y, -ize, -etic etc.)&lt;br&gt;- able to represent new words by breaking down to smaller characters&lt;br&gt;- fit more text within limited context length&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;character tokens&lt;/td&gt;&#xA;          &lt;td&gt;split the tokens to characters&lt;/td&gt;&#xA;          &lt;td&gt;- able to represent new words&lt;/td&gt;&#xA;          &lt;td&gt;- makes modeling difficult&lt;br&gt;- fit less context length&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;byte tokens&lt;/td&gt;&#xA;          &lt;td&gt;breaks down tokens into individual bytes for unicode chars&lt;/td&gt;&#xA;          &lt;td&gt;- can be competitive in multi-lingual scenarios&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;token-embeddings&#34;&gt;&lt;span&gt;Token Embeddings&lt;/span&gt;&#xA;  &lt;a href=&#34;#token-embeddings&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;The tokenization convert the language to a sequence of tokens, and the next step is to find the best numerical representation for these tokens, so the model can use them to calculate and model the &lt;em&gt;patterns&lt;/em&gt; in the text.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;a-language-model-holds-embeddings-for-the-vocabulary-of-its-tokenizer&#34;&gt;&lt;span&gt;A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer&lt;/span&gt;&#xA;  &lt;a href=&#34;#a-language-model-holds-embeddings-for-the-vocabulary-of-its-tokenizer&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;A pre-trained language model is linked with its tokenizer and cannot use a different tokenizer without training. When downloading a pretrained language model, a portion of the model is the embeddings matrix holding the embedding vectors for each token in the tokenizer vocabulary.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;creating-contextualized-word-embeddings-with-language-models&#34;&gt;&lt;span&gt;Creating Contextualized Word Embeddings with Language Models&lt;/span&gt;&#xA;  &lt;a href=&#34;#creating-contextualized-word-embeddings-with-language-models&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;To create better token embeddings than the above static ones, language models create &lt;em&gt;contextualized&lt;/em&gt; word embeddings to represent a word with a &lt;em&gt;different&lt;/em&gt; token based on its context.&lt;/p&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/02-9.PNG&#34; alt=&#34;Contextual embeddings&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig. Static to contextual embeddings.&lt;/div&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;text-embeddings-for-sentences-and-whole-documents&#34;&gt;&lt;span&gt;Text Embeddings (for Sentences and Whole Documents)&lt;/span&gt;&#xA;  &lt;a href=&#34;#text-embeddings-for-sentences-and-whole-documents&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;A SINGLE vector embeding that represents a a longer text piece (e.g., sentence, documents), and the most common way to produce text embedding vector is average the values of all token embeddings.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;word-embeddings-beyond-llms&#34;&gt;&lt;span&gt;Word Embeddings Beyond LLMs&lt;/span&gt;&#xA;  &lt;a href=&#34;#word-embeddings-beyond-llms&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Embeddings can also be useful in domains other than LLM such as &lt;em&gt;recommender engines&lt;/em&gt; and &lt;em&gt;robotics&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Next let&amp;rsquo;s learn about how the embeddings are generated from the &lt;em&gt;Word2Vec&lt;/em&gt; algorithm.&lt;/p&gt;&#xA;&lt;p&gt;How?&lt;/p&gt;&#xA;&lt;p&gt;Train a neural network to predict if words commonly apear in the same context or not (i.e., a classification task).&lt;/p&gt;&#xA;&lt;p&gt;Input:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The embeddings of the words&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Output:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The updated embeddings of the words, so next time the model is presented with the vectors, they have a better chance of being correct based on the true labels (if they are true neighbors)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Training process:&lt;/p&gt;&#xA;&lt;p&gt;Inputs embeddings &amp;ndash;&amp;gt; prediction on labels/target value &amp;ndash;&amp;gt; update the embeddings&lt;/p&gt;&#xA;&lt;p&gt;Key considerations:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;skip-gram&lt;/code&gt;: select neighboring words (e.g., use a sliding window to generate training examples: a central word as one input, and its two neighbor words as second inputs, the model will be trained to classify the neighbors as 1 if it is indeed neighbor otherwise 0)&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;negative sampling&lt;/code&gt;: enrich the training dataset with examples of words that are NOT typically neighbors&lt;/li&gt;&#xA;&lt;li&gt;Other tokenization: how to deal with capitalization, punctuation, and how many tokens in vocabulary&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;embeddings-for-recommendation-systems&#34;&gt;&lt;span&gt;Embeddings for Recommendation Systems&lt;/span&gt;&#xA;  &lt;a href=&#34;#embeddings-for-recommendation-systems&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;RecSys is an example domain that the embeddings can be useful. An example process to make it useful here:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Data loading: Playlists from radio stations.&lt;/li&gt;&#xA;&lt;li&gt;Word2Vec training (treat easch playlist as a sentence, each song in the playlist as a workd/token)&lt;/li&gt;&#xA;&lt;li&gt;Use the trained embeddings to find similar songs&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>1 - An Introduction to Large Language Models</title>
      <link>https://qianqianshan.github.io/posts/an_introduction_to_large_language_models/</link>
      <pubDate>Wed, 03 Sep 2025 03:27:17 -0700</pubDate>
      <guid>https://qianqianshan.github.io/posts/an_introduction_to_large_language_models/</guid>
      <category domain="https://qianqianshan.github.io/categories/hands-on-large-language-models/">Hands-on Large Language Models</category>
      <description>&lt;h2 class=&#34;heading-element&#34; id=&#34;what-is-language-ai&#34;&gt;&lt;span&gt;What Is Language AI?&lt;/span&gt;&#xA;  &lt;a href=&#34;#what-is-language-ai&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;Language AI is a subfield of AI that focuses  on developing technologies capable of understanding, processing, and generating human language.&lt;/p&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;a-recent-history-of-language-ai&#34;&gt;&lt;span&gt;A Recent History of Language AI&lt;/span&gt;&#xA;  &lt;a href=&#34;#a-recent-history-of-language-ai&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;h3 class=&#34;heading-element&#34; id=&#34;representing-language-as-a-bag-of-words&#34;&gt;&lt;span&gt;Representing Language as a Bag-of-Words&lt;/span&gt;&#xA;  &lt;a href=&#34;#representing-language-as-a-bag-of-words&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Tokenization: split up the sentence into individual words or subwords (tokens)&lt;/li&gt;&#xA;&lt;li&gt;Create a vocabulary: combine all unique words from each sentence to create the vocabulary that can be used to represent the sentences&lt;/li&gt;&#xA;&lt;li&gt;Create a bag-of-words by counting how often a word in each sentence appears&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;That is, a bag-of-words model create representations of text in the form of numbers (or vectors), but it &lt;em&gt;ignores the semantic nature or the meaning of the text by considering language as a bag of words&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/01-5.PNG&#34; alt=&#34;Bag-of-words&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig. Bag-of-words flow.&lt;/div&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;better-representations-with-dense-vector-embeddings&#34;&gt;&lt;span&gt;Better Representations with Dense Vector Embeddings&lt;/span&gt;&#xA;  &lt;a href=&#34;#better-representations-with-dense-vector-embeddings&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;&lt;em&gt;word2vec&lt;/em&gt; (released in 2013) attempts to capture the meaning of text in embeddings.&lt;/p&gt;&#xA;&lt;p&gt;Steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use neural network with interconnected layers of nodes to process info.&lt;/li&gt;&#xA;&lt;li&gt;Assign each word a vector embedding with initialized random values.&lt;/li&gt;&#xA;&lt;li&gt;Take pairs of words from the training data, and train model to predict whether they are likely to be neighbors in a sentence.&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;word2vec&lt;/em&gt; learns the relationship between words and distills the info into the embedding.&lt;/li&gt;&#xA;&lt;li&gt;Now if two words tend to have the same neighbors, their embeddings will be closer.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Embeddings attempt to capture meaning by &lt;em&gt;representing the properties of words&lt;/em&gt;, for example, &amp;ldquo;baby&amp;rdquo; may score high on the properties &amp;ldquo;newborn&amp;rdquo; and &amp;ldquo;human&amp;rdquo;, and score low in properties &amp;ldquo;fruit&amp;rdquo;. In practice, the properties are often obscure and seldom relate to a single humanly identifiable concept, but makes sense to translate human language to compute language.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;types-of-embeddings&#34;&gt;&lt;span&gt;Types of Embeddings&lt;/span&gt;&#xA;  &lt;a href=&#34;#types-of-embeddings&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Different levels of embeddings indicate different levels of abstractions (word vs sentence).&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;word embeddings (e.g., word2vec)&lt;/li&gt;&#xA;&lt;li&gt;sentence embeddings&lt;/li&gt;&#xA;&lt;li&gt;document embeddings (e.g., bag-of-words)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;encoding-and-decoding-context-with-attention&#34;&gt;&lt;span&gt;Encoding and Decoding Context with Attention&lt;/span&gt;&#xA;  &lt;a href=&#34;#encoding-and-decoding-context-with-attention&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;How to switch to the dynamic embeddings so the meaning of words is dependent on its &lt;em&gt;context&lt;/em&gt;? For example, &amp;ldquo;bank&amp;rdquo; can refer a financial bank, or the bank of a river.&lt;/p&gt;&#xA;&lt;p&gt;Method 1: Recurrent neural networks (RNNs) - it models the word sequences as an additional input. It encodes to represent an input sentence, and decodes to generate an output sentence. However, it is difficult to deal with longer sentences with one single embedding to represent the entire input.&lt;/p&gt;&#xA;&lt;p&gt;Method 2: Attention feature - To solve the long sentence embedding issue of RNN, attention is added to the &lt;em&gt;decoder step&lt;/em&gt;. It selectively determines which words are most important in a given sentence, and allows a model to focus on parts of the input sequence that are relevant. That is, the input of the generation/decoding step is now the (1). context embedding generated by encoder (2). the hidden states of all input words as signal for each input word related to the potential output.&lt;/p&gt;&#xA;&lt;p&gt;Drawback: Method 1 + 2 has a sequential nature and prevents parallelization during the training of the model.&lt;/p&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;attention-is-all-you-need&#34;&gt;&lt;span&gt;Attention Is All You Need&lt;/span&gt;&#xA;  &lt;a href=&#34;#attention-is-all-you-need&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;A network architecture called &lt;em&gt;Transformer&lt;/em&gt; is&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;solely based on the attention mechanism&lt;/li&gt;&#xA;&lt;li&gt;removed the recurrent network, which makes it ideal for parallel training&lt;/li&gt;&#xA;&lt;li&gt;remains autoregressive - consume each generated word before creating a new word&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Two blocks in the transformer architecture: encoder and decoder&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Common layers: self-attention and feedforward neural network&lt;/li&gt;&#xA;&lt;li&gt;Unique layer of decoder: a layer to pay attention to the output of the encoder&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/01-17.PNG&#34; alt=&#34;Encoder block&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig. Encoder block of transformer.&lt;/div&gt;&#xA;&lt;img src=&#34;https://qianqianshan.github.io/images/hands-on-LLM/01-19.PNG&#34; alt=&#34;Decoder block&#34; width=&#34;50%&#34; /&gt;&#xA;&lt;div style=&#34;text-align: left;&#34;&gt;Fig. Decoder block of transformer.&lt;/div&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;representation-models-encoder-only-models&#34;&gt;&lt;span&gt;Representation Models: Encoder-Only Models&lt;/span&gt;&#xA;  &lt;a href=&#34;#representation-models-encoder-only-models&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Bidirectional Encoder Representations (BERT) is an encoder-only architecture that focuses on &lt;em&gt;representing language&lt;/em&gt;, and the encoder stacks are training with &lt;em&gt;masked language modeling&lt;/em&gt; - masks part of the input for the model to predict, to allow BERT to create better representations of the input.&lt;/p&gt;&#xA;&lt;p&gt;Use case:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Transfer learning: First pretrain (e.g., on entire wikipedia) and then fine-tune it for a specific task (e.g., classification, named entity recognition, clustering, semantic search, etc.)&lt;/li&gt;&#xA;&lt;li&gt;Feature extraction machine: BERT-like models generate embeddings at almost every step in their architecture, thus making extracting features possible without the fine-tuning for a specific task.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 class=&#34;heading-element&#34; id=&#34;generative-models-decoder-only-models&#34;&gt;&lt;span&gt;Generative Models: Decoder-Only Models&lt;/span&gt;&#xA;  &lt;a href=&#34;#generative-models-decoder-only-models&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&lt;p&gt;Focus on generating text like GPT.&lt;/p&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;the-training-paradigm-of-large-language-models&#34;&gt;&lt;span&gt;The Training Paradigm of Large Language Models&lt;/span&gt;&#xA;  &lt;a href=&#34;#the-training-paradigm-of-large-language-models&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;For tradition machine learning tasks, it usually involves training a model for a specific task (e.g., classification) with one-step process. In contrast, creating LLMs usually consists of two steps:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Language modeling (pretraining): Train on a vast corpus of internet text allowing model to learn grammer, context, and language patterns. The output model is a &lt;em&gt;foundational model&lt;/em&gt; or &lt;em&gt;base model&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Fine-tuning (post-training): Further train the previous model on a narrower task.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 class=&#34;heading-element&#34; id=&#34;large-language-model-applications-what-makes-them-so-useful&#34;&gt;&lt;span&gt;Large Language Model Applications: What Makes Them So Useful?&lt;/span&gt;&#xA;  &lt;a href=&#34;#large-language-model-applications-what-makes-them-so-useful&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&lt;p&gt;Common tasks and techniques:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Detecting review sentiment (supervised learning)&lt;/li&gt;&#xA;&lt;li&gt;Developing system to find common topics in ticket issues (unsupervised learning)&lt;/li&gt;&#xA;&lt;li&gt;Building a system for retrieval and inspection of relevant documents (e.g., RAG)&lt;/li&gt;&#xA;&lt;li&gt;Constructing chatbot with external resources&lt;/li&gt;&#xA;&lt;li&gt;Constructing LLM of writing recipes based on fridge food picture (multi-modality)&lt;/li&gt;&#xA;&lt;li&gt;Many more&amp;hellip;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Hello</title>
      <link>https://qianqianshan.github.io/posts/b12471f/</link>
      <pubDate>Sun, 10 Aug 2025 23:50:24 -0700</pubDate>
      <guid>https://qianqianshan.github.io/posts/b12471f/</guid>
      <category domain="https://qianqianshan.github.io/categories/draft/">Draft</category>
      <description></description>
    </item>
  </channel>
</rss>
